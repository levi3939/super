针对1. 订单分批：
• 描述：根据 DeepSeek v2.5 API 的限制，将订单数据按规定的字符数分批处理。
• 目的：避免一次向 API 输入过多字符，导致响应被截断或请求失败。
• 注意：需参考 API 文档（@https://api-docs.deepseek.com/zh-cn/）确定每批的最大字符数。
这些修改实现了以下功能：
新增了split_orders函数，用于将输入的订单数据按照指定的最大字符数（默认4000字符）进行分批。
修改了process_orders函数，使用split_orders函数对输入数据进行分批，然后对每一批数据进行处理。
添加了clean_data_with_api函数的占位符，这个函数将来需要实现调用DeepSeek API的逻辑。
添加了save_to_database函数的占位符，用于将清洗后的数据存入数据库。
5. 新增了log_order_processing函数，用于记录订单处理的日志。
请注意，这些修改还需要进一步完善：
1. clean_data_with_api函数需要实现实际的API调用逻辑。你需要参考DeepSeek API的文档来完成这部分。
save_to_database函数需要实现将清洗后的数据存入MySQL数据库的逻辑。这可能涉及到使用ORM（如SQLAlchemy）或直接使用MySQL连接器。
你可能需要在app.py中相应地修改路由处理函数，以适应新的process_orders函数的使用方式。
确保正确设置了日志记录，可能需要在app.py中初始化日志记录器。

实现调用DeepSeek大模型的逻辑

这些修改实现了以下功能：
我们导入了必要的库，包括OpenAI客户端和用于加载环境变量的dotenv。
我们初始化了OpenAI客户端，使用环境变量中的API密钥和DeepSeek的基础URL。
在clean_data_with_api函数中，我们使用DeepSeek API来清理订单数据。我们发送一个系统消息来设置AI助手的角色，然后发送用户消息包含需要清理的订单数据。
API返回清理后的数据，我们将其作为函数的返回值。
我们添加了错误处理，如果API调用失败，我们会记录错误并返回原始数据。
为了使这段代码正常工作，你需要在项目根目录创建一个.env文件，并在其中添加你的DeepSeek API密钥：
a5262e4a94884ebaa2f2d90a8336cb00
请注意，将API密钥存储在环境变量中是一个好习惯，可以避免直接在代码中暴露敏感信息。
最后，确保在requirements.txt文件中添加以下依赖：
dotenv
这样，当其他人克隆你的项目时，他们可以轻松安装所有必要的依赖。

save_to_database(cleaned_data)

好的，我来帮您实现将清洗后的数据存入数据库的逻辑。根据您提供的文件和需求，我们需要在 utils.py 中实现 save_to_database 函数。这个函数将接收清洗后的数据，解析它，然后将其存储到数据库中。
以下是 save_to_database 函数的实现：
这个实现做了以下几件事：
定义了 parse_order 函数来解析单个订单文本，提取关键信息。
定义了 split_orders 函数来将清洗后的数据拆分为单个订单。
在 save_to_database 函数中：
拆分订单
生成批次ID
解析每个订单并创建 Order 对象
将 Order 对象添加到数据库会话
提交事务，如果出错则回滚
最后关闭数据库会话
添加了 generate_batch_id 函数来生成唯一的批次ID。
请注意，这个实现假设清洗后的数据格式是一致的，每个订单都包含特定的字段。如果实际数据格式有变化，您可能需要调整 parse_order 函数中的正则表达式。
另外，为了提高性能，您可以考虑使用批量插入而不是逐个插入订单。这可以通过收集所有 Order 对象，然后使用 db_session.bulk_save_objects(orders) 来实现。
最后，确保在 utils.py 文件的开头导入所需的模块：
这个实现应该能够满足您的需求，将清洗后的数据解析并存储到数据库中。如果您需要任何进一步的修改或有任何问题，请随时告诉我。